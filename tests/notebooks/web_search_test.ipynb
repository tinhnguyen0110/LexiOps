{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b198c656",
   "metadata": {},
   "outputs": [],
   "source": [
    "from web_search_tool import get_links_from_serper\n",
    "# import trafilatura\n",
    "# from extract_tool import extract_text_with_advanced_bypass\n",
    "# from playwright_tool import test_vietnamese_website_sync\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ad89d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = test_vietnamese_website_sync('https://luatvietnam.vn/quoc-phong/luat-nghia-vu-quan-su-2015-so-78-2015-qh13-moi-nhat-96362-d1.html')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "737ae1f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'success': True,\n",
       " 'error': None,\n",
       " 'method': 'Trafilatura (approach 1)',\n",
       " 'title': 'GitHub - ngothanhnam0910/Vietnamese-Law-Question-Answering-system',\n",
       " 'text': 'In this project, I build a complete Q&A chatbot related to Vietnamese\\'s legal document\\nLink dataset : Link data\\n├── backend\\n│ ├── requirements.txt # backend dependencies for the backend\\n│ ├── entrypoint.sh # script run backend\\n│ ├── src # Source code for the backend\\n│ │ ├── search_document\\n│ │ │ ├── combine_search.py # ensemble result from Bge-m3, e5\\n│ │ │ ├── reranking.py # reranking\\n│ │ │ ├── search_elastic.py # Search using elasticsearch\\n│ │ │ ├── search_with_bge.py # Search using Bge-m3\\n│ │ │ └── search_with_e5.py # Search using Multilingual-e5-large\\n│ │ ├── agent.py # experiment react agent with tool search\\n│ │ ├── app.py # Entry point for the Fast API backend application\\n│ │ ├── brain.py # Logic for intelligent decision-making using OpenAI client\\n│ │ ├── cache.py # Cache implementation for the application\\n│ │ ├── celery_app.py # Celery task queue configuration\\n│ │ ├── config.py # Configuration file for the backend\\n│ │ ├── database.py # Database connection logic\\n│ │ ├── models.py # Database models\\n│ │ ├── tavily_search.py # define tool search internet\\n│ │ ├── schemas.py # Data schemas for API endpoints\\n│ │ ├── task.py # Define task for celery\\n│ │ └── utils.py # Utility functions for the backend\\n├── chatbot-ui # Frontend chatbot application\\n│ ├── chat_interface.py # Chatbot interface logic\\n│ ├── config.toml # Configuration file forchatbot\\n│ ├── entrypoint.sh # Entrypoint script for chatbot\\n│ ├── requirements.txt # Python dependencies for chatbot\\n├── finetune_llm # Directory for finetune llm\\n│ ├── download_model.py # download base model\\n│ ├── finetune.py # finetune LLM for answer generation\\n│ ├── gen_data.py # Code for gen data\\n│ ├── merge_with_base.py # Merge finetuned weight with base model\\n│ └── pdf\\n├── images # Directory for storing image assets\\n├── retrieval # Retrieval folder\\n│ ├── FlagEmbedding # folder include code finetune\\n│ ├── hard_negative_bge_round1.py # search using bge-m3\\n│ ├── hard_negative_e5.py # search using e5\\n│ ├── create_data_rerank.py # create data for reranking\\n│ ├── finetune.sh # Script to finetuning bge-reranker-v2-m3\\n│ └── setup_env.sh # Script to create env\\nTo get starte with this project, we need to do the following\\nInstall all dependencies dedicated to the project in local\\npython -m venv .venv\\nsource .venv/bin/activate\\npip install -r backend/requirements.txt\\npip install -r chatbot-ui/requirements.txt\\nStart application .\\nsh backend/entrypoint.sh\\nsh chatbot-ui/entrypoint.sh\\n- Because the size of each rule is quite long, the first step will need to be chunked into smaller parts. Then these chunks will be passed through 2 embedding models, Bge-m3 and Multilingual-e5-large, and finally these embedding vectors will be stored in Qdrant.\\n- Additionally, these rules are also saved to elasticsearch to enhance the accuracy of retrieval based on lexical matching.\\n-\\nRouting user\\'s intent: Initially, based on the current query and chat history, determine whether this user\\'s intent is chitchat or law topic. The\\ngpt-4o-mini\\nmodel combined withfew-shot prompting\\nis used to perform this intent determination. If the user\\'s intent is chitchat, it will go through openai to return the final answer. Else, going to query reflection step -\\nQuery reflection: Chat history and current query will be rewritten into a single sentence with more complete meaning for easier retrieval. The model used in this step is\\ngpt-4o-mini\\n. -\\nRetrieval Relevant Documents:: The rewritten query will be passed through two embedding models, Bge-m3 and Multilingual-e5-large, then Qdrant will be used to retrieve semantically related documents. Besides elasticsearch is also used for retrieval based on lexical matching. Finally, to avoid losing relevant documents during retrieval, all retrieved documents were merged and duplicates were removed.\\n-\\nReranking:: If reranking is not used, the number of retrieved documents is quite large. If this entire number of documents is put into ChatModel (as Openai), it may exceed the model\\'s input token limit and be expensive. If the number of documents is small (small top_k), it may lead to the loss of related documents. The topk documents retrieved from the previous step will be passed through the rerank model to re-rank the scores and get the top5 documents with the highest scores.\\n-\\nGenerating Final Answer: The LLM combines the top5 documents after reranking step with the user\\'s query and chat history to generate a response. In the prompt for LLM I specified that it will return \\'no\\' if the retrieved document does not contain the answer, so if the response is different from \\'no\\' then it will be the final answer. If the response is \\'no\\' then it will call the search tool in the next step to get more information.\\n-\\nTool call and Generation: Use the Tavily search tool to search for content on the internet related to the query and then feed this content back to LLM to generate answers.\\nCreate enviroment\\ncd retrieval\\nsh setup_env.sh\\n- Train data should be a json file, where each line is a dict like this:\\n{\"query\": str, \"pos\": List[str], \"neg\":List[str]}\\nquery\\nis the query, and pos\\nis a list of positive texts, neg\\nis a list of negative texts.\\n-\\nFor each embedding model => will take the top 25 chunks with the highest similarity to each query. If the chunk is in the labeled data, it will be assigned as positive and vice versa, it will be negative => Then the results of these embedding models will be summarized.\\n-\\nFollow the steps below to create the training dataset\\nStep1: cd retrieval\\nStep2: CUDA_VISIBLE_DEVICES=0 python create_data_rerank.py\\nFinetune BGE-v2-m3 with parameters:\\n- epochs: 6\\n- learning_rate: 1e-5\\n- batch_size = 2\\nRun script for training\\nsh finetune.sh\\n- The training data will be in conversational format.\\n{\"messages\": [{\"role\": \"system\", \"content\": \"You are...\"}, {\"role\": \"user\", \"content\": \"...\"}, {\"role\": \"assistant\", \"content\": \"...\"}]}\\n{\"messages\": [{\"role\": \"system\", \"content\": \"You are...\"}, {\"role\": \"user\", \"content\": \"...\"}, {\"role\": \"assistant\", \"content\": \"...\"}]}\\n{\"messages\": [{\"role\": \"system\", \"content\": \"You are...\"}, {\"role\": \"user\", \"content\": \"...\"}, {\"role\": \"assistant\", \"content\": \"...\"}]}\\n- Follow the steps below to create the training + test dataset\\nStep1: cd finetune_llm\\nStep2: python gen_data.py\\n- The number of training dataset are 10000 samples and the number of test dataset are 1000 samples\\n- The base model I used for finetune is 1TuanPham/T-VisStar-7B-v0.1. This model ranks quite high on the VMLU Leaderboard of Fine-tuned Models\\n- I used the SFTTrainer from trl library to finetune this model. Beside, I user QLora technique to reduce the memory footprint of large language models during finetuning, without sacrificing performance by using quantization.\\nRun script for training\\nCUDA_VISIBLE_DEVICES=0 python finetune.py\\n-\\nMerge weight with model base Run scrip for merge\\npython merge_with_base.py\\nThe evaluation metrics currently in use are:\\n- Recall@k: Evaluate the accuracy of information retrieval\\n- Correctness:The metric evaluates the answer generated by the system to match a given query reference answer.\\nThe golden dataset I chose for evaluation consists of 1000 samples. Each sample includes 3 fields: query, related_documents, answer\\nRecall@k\\n| Model | K=3 | K =5 | K=10 |\\n|---|---|---|---|\\n| BGE-m3 | 55.11% | 63.43% | 72.18% |\\n| E5 | 54.61% | 63.53% | 72.02% |\\n| Elasticsearch | 42.54% | 49.61% | 56.85% |\\n| Ensemble | 68.38% | 74.85% | 80.66% |\\n| Ensemble + rerank | 79.82% | 82,82% | 87.66% |\\nCorrectness\\nScore is rated on a 5-point scale and has an accuracy of 4.27/5',\n",
       " 'author': 'Ngothanhnam',\n",
       " 'date': '2025-01-07',\n",
       " 'url': 'https://github.com/ngothanhnam0910/Vietnamese-Law-Question-Answering-system',\n",
       " 'description': 'Contribute to ngothanhnam0910/Vietnamese-Law-Question-Answering-system development by creating an account on GitHub.'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ssss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c8049e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "link_resutls = get_links_from_serper(\"tội trốn nghĩa vụ quân sự\", 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bb88be2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://tuoitre.vn/cac-thay-doi-ve-thuc-hien-nghia-vu-quan-su-tu-1-7-nguoi-dan-can-luu-y-2025062809455239.htm\n"
     ]
    }
   ],
   "source": [
    "print(link_resutls[4]['url'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "67794eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "filename = \"urls.txt\"\n",
    "def save_results_to_txt(results, filename):\n",
    "    \"\"\"\n",
    "    Ghi danh sách kết quả tìm kiếm vào file txt (mỗi dòng là JSON)\n",
    "    \"\"\"\n",
    "    with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "        for result in results:\n",
    "            f.write(json.dumps(result, ensure_ascii=False) + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0dd01e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_results_to_txt(link_resutls, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7a381d2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://luatvietnam.vn/quoc-phong/luat-nghia-vu-quan-su-2015-so-78-2015-qh13-moi-nhat-96362-d1.html'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "link_resutls[0]['url']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d7ae757c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://luatvietnam.vn/quoc-phong/luat-nghia-vu-quan-su-2015-so-78-2015-qh13-moi-nhat-96362-d1.html\n",
      "Extracting from: https://luatvietnam.vn/quoc-phong/luat-nghia-vu-quan-su-2015-so-78-2015-qh13-moi-nhat-96362-d1.html\n",
      "\n",
      "=== TRAFILATURA METHOD ===\n",
      "Error: fetch_url() got an unexpected keyword argument 'timeout'\n",
      "\n",
      "==================================================\n",
      "\n",
      "=== BEAUTIFULSOUP METHOD ===\n",
      "Error: Request error: 403 Client Error: Forbidden for url: https://luatvietnam.vn/quoc-phong/luat-nghia-vu-quan-su-2015-so-78-2015-qh13-moi-nhat-96362-d1.html\n"
     ]
    }
   ],
   "source": [
    "print(link_resutls[0]['url'])\n",
    "data = compare_extraction_methods(link_resutls[0]['url'])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "crypto_agent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
